{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xElqi-MMBv8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5a4cd0-e0d9-4049-924a-bead6e091f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q langchain-core langchain-community langchain-google-genai duckduckgo-search wikipedia arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "pIMkkx71Ckvu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "pyZh-Va2Djp2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool = Tool(\n",
        "    name=\"DuckDuckGo Search\",\n",
        "    func=DuckDuckGoSearchRun().run,\n",
        "    description=\"User this tool to perform web search using DuckDuckGo\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZpcoMQl7N6VW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "pkjMs5MtPWpV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = [search_tool]"
      ],
      "metadata": {
        "id": "gLUjYm6iPeAt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tool, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxNX9_nDPhcF",
        "outputId": "8e212065-66c1-4c3f-a042-0834841649b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-aac89eea2271>:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(tool, llm, agent=\"zero-shot-react-description\", verbose=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"Indian Budget news\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_ok-6GMHPuKK",
        "outputId": "f86f36ba-5c04-433c-ee0d-98d9a7d8c0be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to search for recent news related to the Indian budget.  I'll use DuckDuckGo to find relevant articles.\n",
            "\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"Indian Budget 2023 news\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\"The focus of the budget is taking everyone together on an inclusive path,\" Sitharaman said, adding that the government is aiming for a fiscal deficit of 4.4% of India's gross domestic ... Here are five key takeaways from India's union budget: ... could be limited since a tiny fraction of Indians pay direct taxes. In 2023, 1.6% of Indians (22.4 million people) actually paid income ... In conclusion, the 2025-2026 Budget is a comprehensive blueprint for India's growth, balancing fiscal prudence with inclusive develop­ment. By focusing on agriculture, MSMEs, green energy, and exports, the government aims to create a robust foundation for long-term economic stability. India's Finance Minister is expected to present a budget with tax cuts for the middle class, aiming to ease the burden of high prices and stagnant wage growth. The budget will also focus on boosting economic growth and fiscal prudence. New Delhi, Jan 31 (PTI) A cut or tweak in income tax rates/slabs ... India's finance minister gave middle class consumers 1 trillion rupees ($11.5 billion) in tax relief in her budget Saturday, seeking to shore up a slowing economy as global risks worsen.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The observation provides some key takeaways from the Indian Budget 2023, but it's a summary and lacks specific details.  I need to refine my search to get more comprehensive information.\n",
            "\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"Indian Union Budget 2023 highlights\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mUnion Finance Minister Nirmala Sitharaman, on February 1, 2023, presented the Union Budget 2023-24 in Parliament. This budget highlighted India's significant achievements and outlined its economic priorities under the theme of \"Saptarishi,\" which emphasises inclusive development, infrastructure, green growth, and youth empowerment. Check all details about Union Budget 2023-24 Vision, Key Highlights, Tax Slabs. ... Union Budget 2023-24 India's Accomplishments. India's rising global profile is because of several accomplishments, such as Aadhaar, Cowin and UPI, Finance Minister Nirmala Sitharaman says in her budget speech. > Highest capital outlay of Rs 2.4 lakh crore for Indian Railways. The highlights of the 2023 budget are as follows: Sitharaman announced ' Saptarishi ' or top seven priorities of the Narendra Modi government during the Amrit Kaal. The list includes inclusive development of all sections of the society, making government's policies reach ... Finance Minister Nirmala Sitharaman Budget Speech, Union Budget 2023 Top 10 Key Highlights : While presenting the Union Budget 2023 in Lok Sabha, Union Finance Minister Nirmala Sitharaman said the Indian economy is on a \"right track and heading towards a bright future\".. \"Our focus on reforms and sound policies resulting in Jan Bhagidari helped us in trying times, our rising global ... Union Budget 2023 Highlights: PDF Download, Key Takeaways, Important Points: Get Union Budget 2023 Highlights & latest News Updates. ... Netherlands Work Visa: All You Need to Know for Indian Applicants. TeamAcko Nov 13, 2024. Delhi Set to Roll Out WhatsApp-based System for Seamless Challan Payments. TeamAcko Nov 8, 2024. Seychelles Visa for ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:This observation gives more specific highlights of the 2023 Indian Union Budget, including key priorities and initiatives.  I think I have enough information to answer the question.\n",
            "\n",
            "Final Answer: The Indian Union Budget 2023, presented by Finance Minister Nirmala Sitharaman, focused on inclusive development, infrastructure, green growth, and youth empowerment under the theme \"Saptarishi.\"  Key highlights included a significant capital outlay for Indian Railways, tax relief for the middle class, and emphasis on initiatives like Aadhaar, Cowin, and UPI.  The budget aimed to balance fiscal prudence with economic growth.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Indian Union Budget 2023, presented by Finance Minister Nirmala Sitharaman, focused on inclusive development, infrastructure, green growth, and youth empowerment under the theme \"Saptarishi.\"  Key highlights included a significant capital outlay for Indian Railways, tax relief for the middle class, and emphasis on initiatives like Aadhaar, Cowin, and UPI.  The budget aimed to balance fiscal prudence with economic growth.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia Search Tool"
      ],
      "metadata": {
        "id": "PIXNIucVQdb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper"
      ],
      "metadata": {
        "id": "5GBnQKgHQhmP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool = Tool(\n",
        "    name=\"Wikipedia Search\",\n",
        "    func=WikipediaAPIWrapper().run,\n",
        "    description=\"User this tool to perform search in wikipedia\",\n",
        ")"
      ],
      "metadata": {
        "id": "RtPKSZ4zPzBJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = [search_tool]"
      ],
      "metadata": {
        "id": "QtQF7jbtQyda"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tool, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "UWMR2YoBQ4P4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"OpenAI\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "UPm9WzcaQ6pW",
        "outputId": "cc3aa6d6-857a-4a82-9570-e88b26c51f51"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find information about OpenAI.  A Wikipedia search should provide a good overview.\n",
            "Action: Wikipedia Search\n",
            "Action Input: OpenAI\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: OpenAI\n",
            "Summary: OpenAI is an American artificial intelligence (AI) research organization founded in December 2015 and headquartered in San Francisco, California. Its stated mission is to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\n",
            "The organization consists of the non-profit OpenAI, Inc., registered in Delaware, and its for-profit subsidiary introduced in 2019, OpenAI Global, LLC. Microsoft owns roughly 49% of OpenAI's equity, having invested US$13 billion. It also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\n",
            "In 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI's products. In November 2023, OpenAI's board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company's prominent role in an industry-wide problem.\n",
            "\n",
            "Page: OpenAI o3\n",
            "Summary: OpenAI o3 is a generative pre-trained transformer (GPT) model developed by OpenAI as a successor to OpenAI o1. It is designed to devote additional deliberation time when addressing questions that require step-by-step logical reasoning.\n",
            "\n",
            "Page: OpenAI o1\n",
            "Summary: OpenAI o1 is a generative pre-trained transformer (GPT). A preview of o1 was released by OpenAI on September 12, 2024. o1 spends time \"thinking\" before it answers, making it better at complex reasoning tasks, science and programming than GPT-4o. The full version was released to ChatGPT users on December 5, 2024.\n",
            "\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  The Wikipedia search provides a comprehensive overview of OpenAI, including its mission, key technologies (GPT models, DALL-E, Sora), funding, legal issues, and leadership changes.\n",
            "Final Answer: OpenAI is an American artificial intelligence research company founded in 2015.  Its mission is to develop safe and beneficial artificial general intelligence.  It's known for models like GPT, DALL-E, and Sora, and has faced challenges including lawsuits and leadership changes.  Microsoft is a major investor.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"OpenAI is an American artificial intelligence research company founded in 2015.  Its mission is to develop safe and beneficial artificial general intelligence.  It's known for models like GPT, DALL-E, and Sora, and has faced challenges including lawsuits and leadership changes.  Microsoft is a major investor.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sqlite Database"
      ],
      "metadata": {
        "id": "wfqy6-qIRtZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3"
      ],
      "metadata": {
        "id": "ODVFFrXKQ_St"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(\"example.db\")"
      ],
      "metadata": {
        "id": "hyAuwT0LRxpq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor = conn.cursor()"
      ],
      "metadata": {
        "id": "j-wixNo4R2uX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS inventory (\n",
        "  id INTEGER PRIMARY KEY,\n",
        "  name TEXT,\n",
        "  quantity INTEGER\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "cursor.executemany(\"\"\"\n",
        "INSERT INTO inventory (name, quantity)\n",
        "VALUES (?, ?)\n",
        "\"\"\", [\n",
        "    (\"Apple\", 10),\n",
        "    (\"Banana\", 20),\n",
        "    (\"Orange\", 15)\n",
        "])\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "_0G4csdQR8ea"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit"
      ],
      "metadata": {
        "id": "PloFreS7R_m_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = SQLDatabase.from_uri(\"sqlite:///example.db\")"
      ],
      "metadata": {
        "id": "dxNNnsQWS_n1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)"
      ],
      "metadata": {
        "id": "i_8swBBVSuXv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = toolkit.get_tools()"
      ],
      "metadata": {
        "id": "M-E7x1FwTYIZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tool, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "A7eyVVOiTcOK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"How many rows are there in the inventory table?\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "wBMnNwqsTesg",
        "outputId": "73380a38-83fc-4aa5-ae1b-401e701ae565"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to get the number of rows in the inventory table.  I'll first check if the table exists using `sql_db_list_tables`. Then I'll construct a SQL query to count the rows.\n",
            "\n",
            "Action: sql_db_list_tables\n",
            "Action Input: \"\"\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3minventory\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The inventory table exists. Now I'll create a SQL query to count the rows.\n",
            "\n",
            "Action: sql_db_query_checker\n",
            "Action Input: SELECT COUNT(*) FROM inventory;\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSELECT COUNT(*) FROM inventory;\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The query checker simply echoed the query.  This means it's syntactically correct. I'll now execute it.\n",
            "\n",
            "Action: sql_db_query\n",
            "Action Input: SELECT COUNT(*) FROM inventory;\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[(3,)]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer. The query returned (3,), indicating there are 3 rows in the inventory table.\n",
            "\n",
            "Final Answer: 3\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"Quanitity of product Orange\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "s2ENX4D5TjM6",
        "outputId": "bb0d8db2-c03b-4b8a-b5fd-3c38d24e7935"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to query the database to find the quantity of the product \"Orange\".  I first need to know the table names to check which table contains the product information.\n",
            "Action: sql_db_list_tables\n",
            "Action Input: \u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3minventory\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: Now that I know the table name, I need to check the schema to see which columns are relevant.\n",
            "Action: sql_db_schema\n",
            "Action Input: inventory\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m\n",
            "CREATE TABLE inventory (\n",
            "\tid INTEGER, \n",
            "\tname TEXT, \n",
            "\tquantity INTEGER, \n",
            "\tPRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from inventory table:\n",
            "id\tname\tquantity\n",
            "1\tApple\t10\n",
            "2\tBanana\t20\n",
            "3\tOrange\t15\n",
            "*/\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I can now construct a SQL query to get the quantity of \"Orange\".\n",
            "Action: sql_db_query_checker\n",
            "Action Input: SELECT quantity FROM inventory WHERE name = 'Orange';\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSELECT quantity FROM inventory WHERE name = 'Orange';\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAction: sql_db_query\n",
            "Action Input: SELECT quantity FROM inventory WHERE name = 'Orange';\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[(15,)]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 15\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'15'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv"
      ],
      "metadata": {
        "id": "v0bnjhpDUuAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun"
      ],
      "metadata": {
        "id": "Qxf-U2r8TqOE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_tool = Tool(\n",
        "    name=\"arxiv\",\n",
        "    func=ArxivQueryRun().run,\n",
        "    description=\"Use this tool to search papers on arxiv. Input should be a query.\",\n",
        ")"
      ],
      "metadata": {
        "id": "3_IZqF0JVH6T"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = [arxiv_tool]"
      ],
      "metadata": {
        "id": "3bVGfPFeVKTa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tool, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "7uM2XWe4VPid"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"Attention is all you need\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "52TpYzsPVRRr",
        "outputId": "9ccf58a6-60ba-482e-81a1-96865ddeeaa8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The question \"Attention is all you need\" is likely referring to the famous paper that introduced the Transformer architecture.  I should search for this paper on arXiv.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: \"Attention is all you need\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-07-22\n",
            "Title: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\n",
            "Authors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\n",
            "Summary: The inference demand for LLMs has skyrocketed in recent months, and serving\n",
            "models with low latencies remains challenging due to the quadratic input length\n",
            "complexity of the attention layers. In this work, we investigate the effect of\n",
            "dropping MLP and attention layers at inference time on the performance of\n",
            "Llama-v2 models. We find that dropping dreeper attention layers only marginally\n",
            "decreases performance but leads to the best speedups alongside dropping entire\n",
            "layers. For example, removing 33\\% of attention layers in a 13B Llama2 model\n",
            "results in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\n",
            "also observe that skipping layers except the latter layers reduces performances\n",
            "for more layers skipped, except for skipping the attention layers.\n",
            "\n",
            "Published: 2021-07-16\n",
            "Title: All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
            "Authors: Chull Hwan Song, Hye Joo Han, Yannis Avrithis\n",
            "Summary: We address representation learning for large-scale instance-level image\n",
            "retrieval. Apart from backbone, training pipelines and loss functions, popular\n",
            "approaches have focused on different spatial pooling and attention mechanisms,\n",
            "which are at the core of learning a powerful global image representation. There\n",
            "are different forms of attention according to the interaction of elements of\n",
            "the feature tensor (local and global) and the dimensions where it is applied\n",
            "(spatial and channel). Unfortunately, each study addresses only one or two\n",
            "forms of attention and applies it to different problems like classification,\n",
            "detection or retrieval.\n",
            "  We present global-local attention module (GLAM), which is attached at the end\n",
            "of a backbone network and incorporates all four forms of attention: local and\n",
            "global, spatial and channel. We obtain a new feature tensor and, by spatial\n",
            "pooling, we learn a powerful embedding for image retrieval. Focusing on global\n",
            "descriptors, we provide empirical evidence of the interaction of all forms of\n",
            "attention and improve the state of the art on standard benchmarks.\n",
            "\n",
            "Published: 2023-06-02\n",
            "Title: RITA: Group Attention is All You Need for Timeseries Analytics\n",
            "Authors: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\n",
            "Summary: Timeseries analytics is of great importance in many real-world applications.\n",
            "Recently, the Transformer model, popular in natural language processing, has\n",
            "been leveraged to learn high quality feature embeddings from timeseries, core\n",
            "to the performance of various timeseries analytics tasks. However, the\n",
            "quadratic time and space complexities limit Transformers' scalability,\n",
            "especially for long timeseries. To address these issues, we develop a\n",
            "timeseries analytics tool, RITA, which uses a novel attention mechanism, named\n",
            "group attention, to address this scalability issue. Group attention dynamically\n",
            "clusters the objects based on their similarity into a small number of groups\n",
            "and approximately computes the attention at the coarse group granularity. It\n",
            "thus significantly reduces the time and space complexity, yet provides a\n",
            "theoretical guarantee on the quality of the computed attention. The dynamic\n",
            "scheduler of RITA continuously adapts the number of groups and the batch size\n",
            "in the training process, ensuring group attention always uses the fewest groups\n",
            "needed to meet the approximation quality requirement. Extensive experiments on\n",
            "various timeseries datasets and analytics tasks demonstrate that RITA\n",
            "outperforms the state-of-the-art in accuracy and is significantly faster --\n",
            "with speedups of up to 63X.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I found several papers with \"Attention\" in the title.  The most likely candidate is the original Transformer paper, but it doesn't seem to be directly returned by this arXiv search.  I need to refine my search.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: \"transformer architecture\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-08-22\n",
            "Title: TurboViT: Generating Fast Vision Transformers via Generative Architecture Search\n",
            "Authors: Alexander Wong, Saad Abbasi, Saeejith Nair\n",
            "Summary: Vision transformers have shown unprecedented levels of performance in\n",
            "tackling various visual perception tasks in recent years. However, the\n",
            "architectural and computational complexity of such network architectures have\n",
            "made them challenging to deploy in real-world applications with\n",
            "high-throughput, low-memory requirements. As such, there has been significant\n",
            "research recently on the design of efficient vision transformer architectures.\n",
            "In this study, we explore the generation of fast vision transformer\n",
            "architecture designs via generative architecture search (GAS) to achieve a\n",
            "strong balance between accuracy and architectural and computational efficiency.\n",
            "Through this generative architecture search process, we create TurboViT, a\n",
            "highly efficient hierarchical vision transformer architecture design that is\n",
            "generated around mask unit attention and Q-pooling design patterns. The\n",
            "resulting TurboViT architecture design achieves significantly lower\n",
            "architectural computational complexity (>2.47$\\times$ smaller than FasterViT-0\n",
            "while achieving same accuracy) and computational complexity (>3.4$\\times$ fewer\n",
            "FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other\n",
            "state-of-the-art efficient vision transformer network architecture designs\n",
            "within a similar range of accuracy on the ImageNet-1K dataset. Furthermore,\n",
            "TurboViT demonstrated strong inference latency and throughput in both\n",
            "low-latency and batch processing scenarios (>3.21$\\times$ lower latency and\n",
            ">3.18$\\times$ higher throughput compared to FasterViT-0 for low-latency\n",
            "scenario). These promising results demonstrate the efficacy of leveraging\n",
            "generative architecture search for generating efficient transformer\n",
            "architecture designs for high-throughput scenarios.\n",
            "\n",
            "Published: 2020-06-15\n",
            "Title: Differentiable Neural Architecture Transformation for Reproducible Architecture Improvement\n",
            "Authors: Do-Guk Kim, Heung-Chang Lee\n",
            "Summary: Recently, Neural Architecture Search (NAS) methods are introduced and show\n",
            "impressive performance on many benchmarks. Among those NAS studies, Neural\n",
            "Architecture Transformer (NAT) aims to improve the given neural architecture to\n",
            "have better performance while maintaining computational costs. However, NAT has\n",
            "limitations about a lack of reproducibility. In this paper, we propose\n",
            "differentiable neural architecture transformation that is reproducible and\n",
            "efficient. The proposed method shows stable performance on various\n",
            "architectures. Extensive reproducibility experiments on two datasets, i.e.,\n",
            "CIFAR-10 and Tiny Imagenet, present that the proposed method definitely\n",
            "outperforms NAT and be applicable to other models and datasets.\n",
            "\n",
            "Published: 2023-11-21\n",
            "Title: Interpretation of the Transformer and Improvement of the Extractor\n",
            "Authors: Zhe Chen\n",
            "Summary: It has been over six years since the Transformer architecture was put\n",
            "forward. Surprisingly, the vanilla Transformer architecture is still widely\n",
            "used today. One reason is that the lack of deep understanding and comprehensive\n",
            "interpretation of the Transformer architecture makes it more challenging to\n",
            "improve the Transformer architecture. In this paper, we first interpret the\n",
            "Transformer architecture comprehensively in plain words based on our\n",
            "understanding and experiences. The interpretations are further proved and\n",
            "verified. These interpretations also cover the Extractor, a family of drop-in\n",
            "replacements for the multi-head self-attention in the Transformer architecture.\n",
            "Then, we propose an improvement on a type of the Extractor that outperforms the\n",
            "self-attention, without introducing additional trainable parameters.\n",
            "Experimental results demonstrate that the improved Extractor performs even\n",
            "better, showing a way to improve the Transformer architecture.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I still haven't found the original \"Attention is all you need\" paper. My searches have been too broad. I will try a more precise search.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: \"attention is all you need transformer\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-05-15\n",
            "Title: Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning\n",
            "Authors: Junfeng Chen, Kailiang Wu\n",
            "Summary: Operator learning for Partial Differential Equations (PDEs) is rapidly\n",
            "emerging as a promising approach for surrogate modeling of intricate systems.\n",
            "Transformers with the self-attention mechanism$\\unicode{x2013}$a powerful tool\n",
            "originally designed for natural language processing$\\unicode{x2013}$have\n",
            "recently been adapted for operator learning. However, they confront challenges,\n",
            "including high computational demands and limited interpretability. This raises\n",
            "a critical question: Is there a more efficient attention mechanism for\n",
            "Transformer-based operator learning? This paper proposes the Position-induced\n",
            "Transformer (PiT), built on an innovative position-attention mechanism, which\n",
            "demonstrates significant advantages over the classical self-attention in\n",
            "operator learning. Position-attention draws inspiration from numerical methods\n",
            "for PDEs. Different from self-attention, position-attention is induced by only\n",
            "the spatial interrelations of sampling positions for input functions of the\n",
            "operators, and does not rely on the input function values themselves, thereby\n",
            "greatly boosting efficiency. PiT exhibits superior performance over current\n",
            "state-of-the-art neural operators in a variety of complex operator learning\n",
            "tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced\n",
            "discretization convergence feature, compared to the widely-used Fourier neural\n",
            "operator.\n",
            "\n",
            "Published: 2025-01-15\n",
            "Title: Attention is All You Need Until You Need Retention\n",
            "Authors: M. Murat Yaslioglu\n",
            "Summary: This work introduces a novel Retention Layer mechanism for Transformer based\n",
            "architectures, addressing their inherent lack of intrinsic retention\n",
            "capabilities. Unlike human cognition, which can encode and dynamically recall\n",
            "symbolic templates, Generative Pretrained Transformers rely solely on fixed\n",
            "pretrained weights and ephemeral context windows, limiting their adaptability.\n",
            "The proposed Retention Layer incorporates a persistent memory module capable of\n",
            "real time data population, dynamic recall, and guided output generation. This\n",
            "enhancement allows models to store, update, and reuse observed patterns across\n",
            "sessions, enabling incremental learning and bridging the gap between static\n",
            "pretraining and dynamic, context sensitive adaptation. The Retention Layer\n",
            "design parallels social learning processes, encompassing attention, retention,\n",
            "reproduction, and motivation stages. Technically, it integrates a memory\n",
            "attention mechanism and episodic buffers to manage memory scalability, mitigate\n",
            "overfitting, and ensure efficient recall. Applications span adaptive personal\n",
            "assistants, real time fraud detection, autonomous robotics, content moderation,\n",
            "and healthcare diagnostics. In each domain, the retention mechanism enables\n",
            "systems to learn incrementally, personalize outputs, and respond to evolving\n",
            "real world challenges effectively. By emulating key aspects of human learning,\n",
            "this retention enhanced architecture fosters a more fluid and responsive AI\n",
            "paradigm, paving the way for dynamic, session aware models that extend the\n",
            "capabilities of traditional Transformers into domains requiring continual\n",
            "adaptation.\n",
            "\n",
            "Published: 2023-06-02\n",
            "Title: RITA: Group Attention is All You Need for Timeseries Analytics\n",
            "Authors: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\n",
            "Summary: Timeseries analytics is of great importance in many real-world applications.\n",
            "Recently, the Transformer model, popular in natural language processing, has\n",
            "been leveraged to learn high quality feature embeddings from timeseries, core\n",
            "to the performance of various timeseries analytics tasks. However, the\n",
            "quadratic time and space complexities limit Transformers' scalability,\n",
            "especially for long timeseries. To address these issues, we develop a\n",
            "timeseries analytics tool, RITA, which uses a novel attention mechanism, named\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I still haven't found the original \"Attention is all you need\" paper.  The search terms are getting too specific and are returning papers that use the phrase in their titles but are not the seminal work.  I will try a different approach. I will search for papers by Vaswani et al., the authors of the original paper.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: \"Vaswani et al. transformer\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2019-10-16\n",
            "Title: Efficiency through Auto-Sizing: Notre Dame NLP's Submission to the WNGT 2019 Efficiency Task\n",
            "Authors: Kenton Murray, Brian DuSell, David Chiang\n",
            "Summary: This paper describes the Notre Dame Natural Language Processing Group's\n",
            "(NDNLP) submission to the WNGT 2019 shared task (Hayashi et al., 2019). We\n",
            "investigated the impact of auto-sizing (Murray and Chiang, 2015; Murray et al.,\n",
            "2019) to the Transformer network (Vaswani et al., 2017) with the goal of\n",
            "substantially reducing the number of parameters in the model. Our method was\n",
            "able to eliminate more than 25% of the model's parameters while suffering a\n",
            "decrease of only 1.1 BLEU.\n",
            "\n",
            "Published: 2024-09-26\n",
            "Title: Transformers, Contextualism, and Polysemy\n",
            "Authors: Jumbly Grindrod\n",
            "Summary: The transformer architecture, introduced by Vaswani et al. (2017), is at the\n",
            "heart of the remarkable recent progress in the development of language models,\n",
            "including widely-used chatbots such as Chat-GPT and Claude. In this paper, I\n",
            "argue that we can extract from the way the transformer architecture works a\n",
            "theory of the relationship between context and meaning. I call this the\n",
            "transformer theory, and I argue that it is novel with regard to two related\n",
            "philosophical debates: the contextualism debate regarding the extent of\n",
            "context-sensitivity across natural language, and the polysemy debate regarding\n",
            "how polysemy should be captured within an account of word meaning.\n",
            "\n",
            "Published: 2020-12-18\n",
            "Title: Stabilizing Transformer-Based Action Sequence Generation For Q-Learning\n",
            "Authors: Gideon Stein, Andrey Filchenkov, Arip Asadulaev\n",
            "Summary: Since the publication of the original Transformer architecture (Vaswani et\n",
            "al. 2017), Transformers revolutionized the field of Natural Language\n",
            "Processing. This, mainly due to their ability to understand timely dependencies\n",
            "better than competing RNN-based architectures. Surprisingly, this architecture\n",
            "change does not affect the field of Reinforcement Learning (RL), even though\n",
            "RNNs are quite popular in RL, and time dependencies are very common in RL.\n",
            "Recently, Parisotto et al. 2019) conducted the first promising research of\n",
            "Transformers in RL. To support the findings of this work, this paper seeks to\n",
            "provide an additional example of a Transformer-based RL method. Specifically,\n",
            "the goal is a simple Transformer-based Deep Q-Learning method that is stable\n",
            "over several environments. Due to the unstable nature of Transformers and RL,\n",
            "an extensive method search was conducted to arrive at a final method that\n",
            "leverages developments around Transformers as well as Q-learning. The proposed\n",
            "method can match the performance of classic Q-learning on control environments\n",
            "while showing potential on some selected Atari benchmarks. Furthermore, it was\n",
            "critically evaluated to give additional insights into the relation between\n",
            "Transformers and RL.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:I found several papers referencing the work of Vaswani et al.  This confirms that the original paper exists and is highly cited. While I haven't directly found the original paper's title and abstract, the references confirm its importance in the field.\n",
            "\n",
            "Final Answer:The question \"Attention is all you need\" refers to a seminal paper introducing the Transformer architecture, authored by Vaswani et al.  While I couldn't directly retrieve the paper's abstract through the provided arXiv tool, numerous subsequent papers cite it as foundational to the field of Transformer networks.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The question \"Attention is all you need\" refers to a seminal paper introducing the Transformer architecture, authored by Vaswani et al.  While I couldn\\'t directly retrieve the paper\\'s abstract through the provided arXiv tool, numerous subsequent papers cite it as foundational to the field of Transformer networks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"Research Paper on Retrieval Augmentation Generation. Give summary of one\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wMLz9lpJVWHZ",
        "outputId": "41dbb66b-f217-4542-b757-2ed346244dde"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to search for research papers on Retrieval Augmentation Generation (RAG) on arXiv.\n",
            "Action: arxiv\n",
            "Action Input: \"Retrieval Augmentation Generation\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-10-30\n",
            "Title: R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\n",
            "Authors: Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen\n",
            "Summary: Retrieval augmented generation (RAG) has been applied in many scenarios to\n",
            "augment large language models (LLMs) with external documents provided by\n",
            "retrievers. However, a semantic gap exists between LLMs and retrievers due to\n",
            "differences in their training objectives and architectures. This misalignment\n",
            "forces LLMs to passively accept the documents provided by the retrievers,\n",
            "leading to incomprehension in the generation process, where the LLMs are\n",
            "burdened with the task of distinguishing these documents using their inherent\n",
            "knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\n",
            "this gap by incorporating Retrieval information into Retrieval Augmented\n",
            "Generation. Specifically, R$^2$AG utilizes the nuanced features from the\n",
            "retrievers and employs a R$^2$-Former to capture retrieval information. Then, a\n",
            "retrieval-aware prompting strategy is designed to integrate retrieval\n",
            "information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\n",
            "where LLMs and retrievers are frozen. Extensive experiments across five\n",
            "datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\n",
            "analysis reveals that retrieval information serves as an anchor to aid LLMs in\n",
            "the generation process, thereby filling the semantic gap.\n",
            "\n",
            "Published: 2022-02-13\n",
            "Title: A Survey on Retrieval-Augmented Text Generation\n",
            "Authors: Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu\n",
            "Summary: Recently, retrieval-augmented text generation attracted increasing attention\n",
            "of the computational linguistics community. Compared with conventional\n",
            "generation models, retrieval-augmented text generation has remarkable\n",
            "advantages and particularly has achieved state-of-the-art performance in many\n",
            "NLP tasks. This paper aims to conduct a survey about retrieval-augmented text\n",
            "generation. It firstly highlights the generic paradigm of retrieval-augmented\n",
            "generation, and then it reviews notable approaches according to different tasks\n",
            "including dialogue response generation, machine translation, and other\n",
            "generation tasks. Finally, it points out some important directions on top of\n",
            "recent methods to facilitate future research.\n",
            "\n",
            "Published: 2024-07-04\n",
            "Title: Meta-prompting Optimized Retrieval-augmented Generation\n",
            "Authors: João Rodrigues, António Branco\n",
            "Summary: Retrieval-augmented generation resorts to content retrieved from external\n",
            "sources in order to leverage the performance of large language models in\n",
            "downstream tasks. The excessive volume of retrieved content, the possible\n",
            "dispersion of its parts, or their out of focus range may happen nevertheless to\n",
            "eventually have a detrimental rather than an incremental effect. To mitigate\n",
            "this issue and improve retrieval-augmented generation, we propose a method to\n",
            "refine the retrieved content before it is included in the prompt by resorting\n",
            "to meta-prompting optimization. Put to empirical test with the demanding\n",
            "multi-hop question answering task from the StrategyQA dataset, the evaluation\n",
            "results indicate that this method outperforms a similar retrieval-augmented\n",
            "system but without this method by over 30%.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I have retrieved several papers on Retrieval Augmentation Generation from arXiv.  I will summarize one of them.\n",
            "Thought: I will summarize the paper \"R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\".\n",
            "Final Answer: The paper \"R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\" addresses the semantic gap between Large Language Models (LLMs) and retrievers in Retrieval Augmented Generation (RAG).  This gap causes LLMs to passively use retrieved documents, hindering comprehension.  R^2AG improves RAG by incorporating retrieval information using a novel R^2-Former and a retrieval-aware prompting strategy. This allows the LLM to better understand and utilize the retrieved information, particularly beneficial in low-source scenarios where the LLM and retriever are frozen. Experiments across five datasets show R^2AG's effectiveness, robustness, and efficiency.  The retrieval information acts as an anchor, bridging the semantic gap and improving generation.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper \"R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\" addresses the semantic gap between Large Language Models (LLMs) and retrievers in Retrieval Augmented Generation (RAG).  This gap causes LLMs to passively use retrieved documents, hindering comprehension.  R^2AG improves RAG by incorporating retrieval information using a novel R^2-Former and a retrieval-aware prompting strategy. This allows the LLM to better understand and utilize the retrieved information, particularly beneficial in low-source scenarios where the LLM and retriever are frozen. Experiments across five datasets show R^2AG\\'s effectiveness, robustness, and efficiency.  The retrieval information acts as an anchor, bridging the semantic gap and improving generation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Agent"
      ],
      "metadata": {
        "id": "LWoNYXCYWggu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.file_management import ReadFileTool"
      ],
      "metadata": {
        "id": "vVxIBhImVj0c"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_file_tool = Tool(\n",
        "    name=\"read_file\",\n",
        "    func=ReadFileTool().run,\n",
        "    description=\"Use this tool to read a file. Input should be the file path.\",\n",
        ")"
      ],
      "metadata": {
        "id": "t8LHuvleWxt7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = [read_file_tool]"
      ],
      "metadata": {
        "id": "lMfnDQw0W0pq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tool, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "ng025eG_W3IX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"001ssb.txt\"\n",
        "\n",
        "response = agent.run(f\"Summary of {file_name}\")\n",
        "response"
      ],
      "metadata": {
        "id": "knLpd4eIW7va",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "11d55d39-44ce-4d22-be6a-b8acef2b38e0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to read the content of the file 001ssb.txt to create a summary.\n",
            "Action: read_file\n",
            "Action Input: '001ssb.txt'\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mError: no such file or directory: '001ssb.txt'\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I cannot access the file '001ssb.txt'.  Therefore, I cannot provide a summary.\n",
            "Final Answer: I am unable to provide a summary because the file '001ssb.txt' could not be found.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I am unable to provide a summary because the file '001ssb.txt' could not be found.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAGxKq2NXBUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}